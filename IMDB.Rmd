---
title: "R IMDB Webscraping"
output:
  word_document: default
  html_document:
    df_print: paged
  pdf_document: default
---

Guide: <https://www.analyticsvidhya.com/blog/2017/03/beginners-guide-on-web-scraping-in-r-using-rvest-with-hands-on-knowledge/>

I'll be using the guide listed above on the 2020 IMDB movie list in order to learn the basics of webscraping with the rvest library. I'll also be using the dpylr and tidyverse libraries.

## Libraries
```{r libraries, error=FALSE, message=FALSE}
library(rvest)
library(dplyr)
library(tidyverse)

#EDA Libraries
library(reshape2)

#ML Libraries
library(neuralnet)
library(caret)
```

## URL and Data Loading

The data for this project will come from IMDB, specifically the sites 2020 Feature Films list (sorted by popularity). We'll save the URL as a string variable, and load the site to a list variable with the read_html() command.

```{r Loading}
url <- 'https://www.imdb.com/search/title/?count=100&release_date=2020,2020&title_type=feature'

#Reading HTML code from site
webpage <- read_html(url)
```

## Scraping

Now that we have a copy of the site saved in R, we can start pulling the data we need from it. Since most theaters were closed in the year 2020 due to the COVID Pandemic, we won't be looking at Gross Earnings. We'll still be looking at other variables though. Specifically, we want to pull the data for: rankings, titles, descriptions, runtimes, genre, metascores, ratings, votes, directors, and actors. To pull the data, I used the SelectorGadget Chrome Extension tool to find out which bit of HTML code was needed to locate the data within the saved webpage.

For the rankings, it was as simple as loading in the data, seeing how the output looked, and converting that output to a numeric from a string.
```{r Rankings}
ranking_html <- html_nodes(webpage, '.text-primary')
rank_data <- html_text(ranking_html)
head(rank_data)

ranking <- as.numeric(rank_data)
head(ranking)
```

Titles were even easier to load, because they were already set as the correct data-type.

```{r Titles}
titles_html <- html_nodes(webpage,'.lister-item-header a')
title_data <- html_text(titles_html)
head(title_data)

```

For the movie descriptions, I used gsub to remove all the newline marks, and looked at the data again to make sure it looked right.

```{r Description}
descriptions_html <- html_nodes(webpage,'.ratings-bar+ .text-muted')
description_data <- html_text(descriptions_html)
head(description_data)
description_data <- gsub("\n", "", description_data)
head(description_data)
```

When scraping out the runtimes, I had to remove the "min" suffix and convert to a numeric.

```{r Runtime}
runtimes_html <- html_nodes(webpage,'.runtime')
runtime_data <- html_text(runtimes_html)
head(runtime_data)
runtime_data <- as.numeric(gsub(" min", "", runtime_data))
head(runtime_data)
```

While scraping the genres, I realized that because most movies fall under multiple genres analysis would be difficult. To cope with this, I kept only the first genre listed as that would typically be it's main category. I then converted the genres to a factor, to make later analysis easier.

```{r Genre}
genres_html <- html_nodes(webpage, '.genre')
genres_data <- html_text(genres_html)
head(genres_data)
#Remove \n and spaces, keep only first genre, and convert to factor
genres_data <- gsub("\n","",genres_data)
genres_data <- gsub(" ", "", genres_data)
genres_data <- as.factor(gsub(",.*", "", genres_data))
head(genres_data)
```

When pulling out the metascore data, there were some movies that didn't have any. After scrolling through the site to figure out which ones they were, I replaced their metascore data with NA values and made sure that I had the right amount of data.

```{r Metascore}
metascore_html <- html_nodes(webpage, '.metascore')
metascore_data <- html_text(metascore_html)
head(metascore_data)
metascore_data <- gsub(" ", "", metascore_data)
head(metascore_data)
#7 Movies don't have metascore data. numbers: 3, 27, 41, 45, 48, 69, 77
length(metascore_data)
for(i in c(3, 27, 41, 45, 48, 69, 77)){
  a<-metascore_data[1:(i-1)]
  b<-metascore_data[i:length(metascore_data)]
  metascore_data<-append(a,list("NA"))
  metascore_data<-append(metascore_data,b)
}
metascore_data <- as.numeric(metascore_data)
head(metascore_data)
length(metascore_data)
```

Movie rating's were another easy variable to scrape. I just had to convert to a numeric.

```{r Rating}
ratings_html <- html_nodes(webpage,'.ratings-imdb-rating strong')
rating_data <- html_text(ratings_html)
head(rating_data)
rating_data <- as.numeric(rating_data)
head(rating_data)
```

As with runtimes, I had to clean the data (by removing commas instead of suffixes) before converting to a numeric.

```{r Votes}
votes_html <- html_nodes(webpage,'.sort-num_votes-visible span:nth-child(2)')
vote_data <- html_text(votes_html)
head(vote_data)
vote_data <- as.numeric(gsub(",", "", vote_data))
head(vote_data)
```

Directors were saved as a factor as well.

```{r Director}
director_html <- html_nodes(webpage, '.text-muted+ p a:nth-child(1)')
director_data <- html_text(director_html)
head(director_data)
director_data <- as.factor(director_data)
```

Actors were also saved as factors.

```{r Actor}
actor_html <- html_nodes(webpage, '.lister-item-content .ghost+ a')
actor_data <- html_text(actor_html)
head(actor_data)
actor_data <- as.factor(actor_data)
```

## Assembling Dataframe

Once all the data is saved as individual variables, we can put them all together into one dataframe and call the str() function to make sure we have eveything in the right format.

```{r Dataframe}
movies <- data.frame(Rank = rank_data, Title = title_data, 
                     Description = description_data, Runtime = runtime_data,
                     Genre = genres_data, Rating = rating_data,
                     Metascore = metascore_data, Votes = vote_data,
                     Director = director_data, Actor = actor_data)
str(movies)
movies$Rank <- as.numeric(movies$Rank)
str(movies)
```
## EDA and Plots

Now that we have a nice looking dataframe, it's time to make some simple graphs looking at the different relationships. 

```{r Early_EDA, warning= FALSE}
ggplot(movies, aes(x = Runtime, fill = Genre)) + geom_histogram(bins = 30)
ggplot(movies, aes(x = Rating, fill = Genre)) + geom_histogram(bins = 30)
ggplot(movies, aes(x = Rating, y = Runtime)) + 
  geom_point(aes(color = Genre, size = Metascore)) + geom_smooth()
ggplot(movies, aes(x = Metascore, y = Rating)) + 
  geom_point(aes(color = Genre, size = Votes)) + geom_smooth()
```

At a glance, We can see that most movies released in 2020 had a runtime of about 100 minutes and tended to be rated near 6.5. We can also see a slight positive correlation between  rating and runtime, and a much stronger one between Metascore and Rating. To get a deeper look at these relationships, we can strip all non-numeric and variables with NA's from the data and plot the correlation values of each variables interactions.

When making the tile plots, R removes all values for the correlations with an NA in one category, which means our Metascores are just blank correlations. To cope with this, I'll subset the data and remove the rows with an NA. 


```{r Correlation}

plot(movies[, -c(2, 3, 5, 9, 10)])

correlated <- movies[, -c(2, 3, 5, 9, 10)] %>%
  cor() %>% 
  melt() 

ggplot(correlated, aes(x = Var1, y = Var2, fill = value)) + 
  scale_fill_gradient2(low = "antiquewhite", mid = "gold", high = "firebrick") +
  geom_tile(color = "black") + 
  geom_text(aes(label = round(value, 2)), size = 4) +
  theme(axis.text.x = element_text(angle = 90), axis.title = element_blank()) + 
  labs(title = "Correlation of 2020 Feature Films (With Metascores")

#Metascore is removed for the tile plot because the NA values prevent an accurate assessment of correlation.
correlated <- movies[, -c(2, 3, 5, 7, 9, 10)] %>%
  cor() %>% 
  melt() 

ggplot(correlated, aes(x = Var1, y = Var2, fill = value)) + 
  scale_fill_gradient2(low = "antiquewhite", mid = "gold", high = "firebrick") +
  geom_tile(color = "black") + 
  geom_text(aes(label = round(value, 2)), size = 4) +
  theme(axis.text.x = element_text(angle = 90), axis.title = element_blank()) +  
  labs(title = "Correlation of 2020 Feature Films (Without Metascores)")


correlated_noNA <- subset(movies, is.na(Metascore) == FALSE) 
correlated_noNA <- correlated_noNA[, -c(2, 3, 5, 9, 10)] %>%
  cor() %>% 
  melt() 

ggplot(correlated_noNA, aes(x = Var1, y = Var2, fill = value)) + 
  scale_fill_gradient2(low = "antiquewhite", mid = "gold", high = "firebrick") +
  geom_tile(color = "black") + 
  geom_text(aes(label = round(value, 2)), size = 4) +
  theme(axis.text.x = element_text(angle = 90), axis.title = element_blank()) +
  labs(title = "Correlation of 2020 Films (No NAs)")

```

We can see through these plots that a movies rating and metascore correlate highly. Votes seem to have a consistent correlation with the other factors, although it's an inverse correlation with Rank. Rank actually only seems to have negative correlations, which is worrisome.


## Deeper Analysis

The goal is to predict a movies rating based on generally applicable factors such as runtime, genre, metascore, and IMDB votes. To do this we'll use a neural network. We'll start by setting our seed so that the data will be reproducible, and removing the 7 rows with an NA for a metascore. Next we use the model.matrix() function to convert our categorical variable (genre) into multiple binary columns. Then using a quick custom function to normalize the data, we split it into our test and train groups and double check the structure of the training group to ensure there won't be any problems.

```{r Seeding_and_Scaling}

set.seed(37) #Set the seed so that the data is reproducible
head(movies)
df <- subset(movies, is.na(Metascore) == FALSE)
str(df) #subset original frame and make sure it looks good
dummys <- model.matrix(~ Rating + Runtime + Metascore + Votes + Genre, data = df)
dummys <- as.data.frame(dummys)

normalize <- function(x) {
  return ((x - min(x)) / (max(x) - min(x)))
}

maxmindf <- as.data.frame(lapply(dummys, normalize))

scaleddata <- as.data.frame(maxmindf)

ind <- sample(2, nrow(scaleddata), replace = TRUE, prob = c(0.7, 0.3)) #Randomly assign indexes with ~80% as 1 (training data)
train <- scaleddata[ind == 1,] #training data is everything with an index of 1
test <- scaleddata[ind == 2,] #testing data is everything with an index of 2
str(train)
```

Now that our data is ready to go, we create the neural network for our training data, and save it as variable nn. This will allow us to call it later for our testing data.

```{r Neuralnet}
nn <- neuralnet(Rating ~ Runtime + Metascore + Votes + GenreAdventure +
                  GenreAnimation + GenreBiography + GenreComedy + GenreCrime +
                  GenreDrama + GenreFantasy + GenreHorror + GenreMystery, 
                data = train, hidden = c(5,5), linear.output = FALSE, threshold = 0.01)
plot(nn)
```

Once our neural network is trained, we can run the testing data through and see how accurate it is. We use the compute() function to save the test results to a new variable which has its value rounded to make it easier to read before being converted from a temporary table to a new dataframe. We then use the table() function to look at the total number of true positive, false positives, true negatives, and false negatives.

```{r Predictions}
nn_results <- compute(nn, test)
results <- data.frame(Actual = test$Rating, Predicted = nn_results$net.result)
roundedresults<-sapply(results,round,digits=0)
roundedresultsdf=data.frame(roundedresults)
table(roundedresultsdf$Actual,roundedresultsdf$Predicted)


#Descaling
predicted=results$Predicted * abs(diff(range(dummys$Rating))) + min(dummys$Rating)
actual=results$Actual * abs(diff(range(dummys$Rating))) + min(dummys$Rating)
comparison=data.frame(predicted,actual)
deviation=((actual-predicted)/actual)
comparison=data.frame(actual,predicted,deviation)
comparison
accuracy=1-abs(mean(deviation))
accuracy
```

From our newest table, we can see that the neural net was correct in 20 of its 26 guesses (15 true positives and 5 true negatives). And if we compare the ratings to the predicted results after de-normalization, we get an accuracy of ~97.7%. With another dataset, results like this would be great. However, our data includes variables such as metascore and votes which can't be decided in film production. This means that, as accurate as this model is, it's not useful.

