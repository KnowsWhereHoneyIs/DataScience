---
title: "Wine Quality Prediciton"
author: "William Lovejoy"
date: "3/4/2022"
output: word_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(scales)
library(dplyr)
library(stats)
library(reshape2)
library(randomForest)
library(caret)
```

## Data Loading and Exploration

Data was accquired from <https://www.kaggle.com/yasserh/wine-quality-dataset>. As always, load it in, and look at the head and types of data in the data set. I prefer using glimpse() as it lets me see the column name, data type, and some of the data. Lastly, I want to know what variables are the biggest determinants in percieved wine quality. So I want a general idea of how the wine quality scores are distributed


```{r data}
df <- read.csv("C:\\Users\\William Lovejoy\\Documents\\Codes\\R\\DataScience\\Wine\\WineQT.csv")
head(df)
glimpse(df)
ggplot(df, aes(y = quality)) + geom_bar()
```

Mostly 5's and 6's. So let's look at overall relationship between the variables. To start with, we'll make a simple scatterplot of all the variables against each other (with the exception of the ID category)

## Plots


```{r plotting}
plot(df[,-c(13)])
```

Quality seems to have a positive relationship with alcohol, and a negative relationship with volatile acidity. So lets look at those in more detail later. For now though, we can make quick smooth plots for all of the variables vs. quality.
```{r smooths, echo= FALSE}
ggplot(df, aes(x = fixed.acidity, y = quality)) + geom_smooth()
ggplot(df, aes(x = volatile.acidity, y = quality)) + geom_smooth()
ggplot(df, aes(x = citric.acid, y = quality)) + geom_smooth()
ggplot(df, aes(x = residual.sugar, y = quality)) + geom_smooth()
ggplot(df, aes(x = chlorides, y = quality)) + geom_smooth()
ggplot(df, aes(x = free.sulfur.dioxide, y = quality)) + geom_smooth()
ggplot(df, aes(x = total.sulfur.dioxide, y = quality)) + geom_smooth()
ggplot(df, aes(x = density, y = quality)) + geom_smooth()
ggplot(df, aes(x = pH, y = quality)) + geom_smooth()
ggplot(df, aes(x = sulphates, y = quality)) + geom_smooth()
ggplot(df, aes(x = alcohol, y = quality)) + geom_smooth()
```

As expected, strong negative association with volatile acidity and a strong positive associatin with alcohol. There are some other variables that lead to a fairly positive association at least within certain ranges. Citric acid levels between 0.37 and 0.75 have higher quality scores, and sulphate values correlate positively with quality if they're under 0.8.

Now we that we have the basic exploration done, we can make a more complex chart. We'll start by calculating correlation valeus for all the variables except for ID, and then use melt() to reshape the data frame into just 3 columns (variable 1, variable 2, and correlation score)
```{r tile}
correlated <- df[, -c(13)] %>% #column 13 is the ID numbers, so we'll drop that here.
  cor() %>% 
  melt() 

ggplot(correlated, aes(x = Var1, y = Var2, fill = value)) + scale_fill_gradient2(low = "antiquewhite",
                                                                                 mid = "gold",
                                                                                 high = "firebrick") +
  geom_tile(color = "black") + 
  geom_text(aes(label = round(value, 2)), size = 2) +
  theme(axis.text.x = element_text(angle = 90), axis.title = element_blank()) + 
  labs(title = "Correlation of Wine Factors with Quality") 

```

Just like before, but now a bit more clear. Quality is positively correlated with alcohol, sulphates, citric acid, and fixed acidity. It also has a noticeable negative correlation with volatile acidity.

Now that we know what sort of data is important, we can try and train a model to predict important variables. As this is my first foray into machine learning. The goal is to predict the quality of wine based on the listed variables using a Random Forest model. I'm using Random Forest due to it's accuracy, ease of use, and how well it fits with my data and goals (prediction with regression)

I set the seed so the data will be reproducible, and convert my quality scores into factors. I then split my data into it's train and testing groups (80/20), so everything will be ready to go.

```{r seeding}
set.seed(100) #Set the seed so that the data is reproducible
df$quality <- as.factor(df$quality) #Make the quality scores a factor
ind <- sample(2, nrow(df), replace = TRUE, prob = c(0.8, 0.2)) #Randomly assign indexes with ~80% as 1 (training data)
train <- df[ind == 1,] #training data is everything with an index of 1
test <- df[ind == 2,] #testing data is everything with an index of 2
```

Now I run my random forest, and get an error rate of 34.89%. That means that my model has about 66.96% accuracy. I'll also make my prediction and confusion matrices here.

```{r rf}
forest <- randomForest(data = train[,-c(13)], quality ~ ., proximity = TRUE) #runs the data through the random forest model
print(forest)

#Confusion and Prediction Matrices
p1 <- predict(forest, train)
print("Training Matrices")
confusionMatrix(p1, train$quality)

p2 <- predict(forest, test) 
print("Testing Matrices")
confusionMatrix(p2, test$quality) 

plot(forest)
```

Our training accuracy was a 1, so everything trained correctly. Going ahead and running the test gives us an accuracy of 0.6992 or ~69.92%. This is a bit lower than I was expecting, so I'll try and fine tune the forest model. The goal here is to find the best value to input as mtry in my random forest model. This value determines how many variables it attempts at each for in the pathway

```{r finetuning}

mtry <- tuneRF(df[, -c(13)], df$quality, ntreeTry=500,
               stepFactor=1.5,improve=0.01, trace=TRUE, plot=TRUE)
best.m <- mtry[mtry[, 2] == min(mtry[, 2]), 1]
print(mtry)
print(best.m)
```

mtry = 9 seems to be golden. It has an OOB of 0. So we'll run our random forest again with an mtry of 9. We'll also look at how important each variable is when it comes to calculating the quality of wine. A higher MeanDecreaseGini means that the variable is more important when calculating the quality of a wine. Alcohol is the most important variable, followed by volatile acidity, sulphates, and total sulphur dioxide. This reflects our tile plot from the beginning.

```{r retry}

forest2 <- randomForest(data = train[,-c(13)], quality ~ ., proximity = TRUE, mtry = 9) #runs the data through the random forest model
print(forest2)

#Matrices
p12 <- predict(forest2, train) 
confusionMatrix(p12, train$quality)


p22 <- predict(forest2, test)
confusionMatrix(p22, test$quality)


plot(forest2)

importance(forest2) 
varImpPlot(forest2)

```

```{r tile2,echo = FALSE}
ggplot(correlated, aes(x = Var1, y = Var2, fill = value)) + scale_fill_gradient2(low = "antiquewhite",
                                                                                 mid = "gold",
                                                                                 high = "firebrick") +
  geom_tile(color = "black") + 
  geom_text(aes(label = round(value, 2)), size = 2) +
  theme(axis.text.x = element_text(angle = 90), axis.title = element_blank()) + 
  labs(title = "Correlation of Wine Factors with Quality") 
```

It's important to note that while volatile acidity has a strong impact on wine quality score, it's a negative impact. The plot of variable importance does not show which variable we want more of, just which ones have the largest impact. By comparing these variables to the tile plot from before, we can infer that people like their wine to be stronger, with low to moderate amounts of sulphates and low levels of volatile acidity.